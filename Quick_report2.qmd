---
title: "Quick Report 2"
author: 'Tayafa and Jose'
date: today
format:
  html: 
    dpi: 300
    embed-resources: true
execute:
  warning: false
  error: false
---
# Assessing nonbank's reaction after (Bank Term Funding Program) BTFP

Our quick report is inspired by the following article [Bond Funds in the Aftermath of SVB's Collapse](https://libertystreeteconomics.newyorkfed.org/2023/11/bond-funds-in-the-aftermath-of-svbs-collapse/)

After the Silicin Valley Bank collapse in March, 2023 the Federal Reserve system authorized the BTFP program. It is a loan program for banks that was established to help stabilize the banking industry after 2023 banking crisis. The eligible banks can borrow up to one year against proper collateral. The article is explianing how the SVB and BTFP events affect the bond funds mostly the open end bond funds as they are more prone to liquidity risk.


```{python}
import pandas as pd
import altair as alt
import numpy as np
```



```{python}
# Load the dataset
CRSP = pd.read_csv('CRSP_ETF_73.csv')
```

```{python}
# Skip the first 4 rows and set row 5 as the header
ETF_FI = pd.read_excel('ETF_FI.xlsx', skiprows=4)
```

```{python}
# Perform the merge to keep only matching tickers from the CSV file
# Assuming both datasets have the 'TICKER' column
merged_data1 = pd.merge(CRSP, ETF_FI[['TICKER']], on='TICKER', how='inner')

# Save the result to a new CSV file (optional)
merged_data1.to_csv('matched_CRSP_ETF.csv', index=False)
```

```{python}
# Get the number of unique tickers in the TICKER column
num_unique_tickers = CRSP['TICKER'].nunique()

# Print the result
print(f"Number of unique tickers: {num_unique_tickers}")
```

```{python}
# Get the number of unique tickers in the TICKER column
num_unique_tickers = merged_data1['TICKER'].nunique()

# Print the result
print(f"Number of unique tickers: {num_unique_tickers}")
```


### Chart 1
```{python}
# Assuming 'merged_data1' is your main DataFrame with relevant columns
# Filter to keep only the relevant columns
columns_to_keep = ['date', 'VOL', 'PRC']
df_filtered = merged_data1[columns_to_keep]

# Convert 'date' column to datetime format
df_filtered['date'] = pd.to_datetime(df_filtered['date'])

# Filter data from February 2023 onward
df_filtered = df_filtered[df_filtered['date'] >= '2023-01-01']

# Calculate daily price change
df_filtered['price_change'] = df_filtered['PRC'].diff()

# Calculate net flows as (Volume * Price Change) for an approximation
df_filtered['net_flow'] = df_filtered['VOL'] * df_filtered['price_change']

# Aggregate by week to smooth the data for clearer visualization
df_weekly = df_filtered.resample('W-Mon', on='date').sum().reset_index()

# Disable the max row limit in Altair
alt.data_transformers.disable_max_rows()

# Create a highlight for March 2023
highlight = alt.Chart(pd.DataFrame({
    'date': ['2023-03-01', '2023-03-31']
})).mark_rect(opacity=0.2, color='yellow').encode(
    x=alt.X('date:T', title='')
)

# Create the main chart with weekly aggregated data and format x-axis for month names
chart = alt.Chart(df_weekly).mark_bar().encode(
    x=alt.X('date:T', title='', axis=alt.Axis(format='%B')),  # Show only month name on x-axis
    y=alt.Y('net_flow:Q', title='Billions of dollars', axis=alt.Axis(format='~s')),
    color=alt.condition(
        alt.datum.net_flow < 0, alt.value('#ff6961'), alt.value('#2b6cb0')  # Red for outflows, blue for inflows
    )
).properties(
    title='Fixed Income Funds Net Flows (Volume-Based), Highlighting March 2023'
)

# Combine the highlight with the main chart
final_chart = highlight + chart

# Display the chart
final_chart
```


### Chart 2
```{python}
# Step 1: Identify outflow days (net_flow < 0) and mark them
df_filtered['outflow'] = df_filtered['net_flow'] < 0

# Step 2: Calculate the run lengths of consecutive outflow days
# Create a column that increments with each new sequence of outflows
df_filtered['outflow_group'] = (df_filtered['outflow'] != df_filtered['outflow'].shift()).cumsum()

# Filter for only the outflow days and group by each outflow episode
outflow_durations = (
    df_filtered[df_filtered['outflow']]
    .groupby('outflow_group')
    .size()
    .reset_index(name='duration')
)

# Calculate the frequency of each duration
duration_counts = outflow_durations['duration'].value_counts().reset_index()
duration_counts.columns = ['duration', 'count']

# Calculate the total count to normalize
total_count = duration_counts['count'].sum()

# Add a column for density
duration_counts['density'] = duration_counts['count'] / total_count

# Step 3: Plot the normalized histogram (density) of outflow episode durations
chart = alt.Chart(duration_counts).mark_bar().encode(
    x=alt.X('duration:O', title='Run Length (Days)'),  # No binning, display as ordinal values
    y=alt.Y('density:Q', title='Density of Episodes'),  # Use density as the y-axis
    color=alt.value('#2b6cb0')
).properties(
    title='Fixed Income Outflow Episodes by Duration (Normalized Density)'
).configure_title(
    fontSize=16,
    anchor='start',
    font='Arial'
).configure_axis(
    labelFontSize=12,
    titleFontSize=12
)

# Display the chart
chart

```


### Chart 3 (making the dataset) 
```{python}
# Skip the first 4 rows and set row 5 as the header
Corp = pd.read_excel('Corporate.xlsx', skiprows=4)
```

```{python}
# Perform the merge to keep only matching tickers from the CSV file
# Assuming both datasets have the 'TICKER' column
merged_data2 = pd.merge(CRSP, Corp[['TICKER']], on='TICKER', how='inner')

# Remove the matched tickers from CRSP to leave only equity funds and unmatched assets
CRSP_no_bonds = CRSP[~CRSP['TICKER'].isin(merged_data1['TICKER'])]

# Save the result to a new CSV file (optional)
merged_data2.to_csv('matched_CRSP_Corp.csv', index=False)
CRSP_no_bonds.to_csv('CRSP_no_bonds.csv', index=False)  # Equity funds without corporate bonds
```

```{python}
# Get the number of unique tickers in the TICKER column
num_unique_tickers = merged_data2['TICKER'].nunique()

# Print the result
print(f"Number of unique tickers: {num_unique_tickers}")
```

```{python}
# Get the number of unique tickers in the TICKER column
num_unique_tickers = CRSP_no_bonds['TICKER'].nunique()

# Print the result
print(f"Number of unique tickers: {num_unique_tickers}")
```



```{python}
# Function to calculate daily net flow as a percentage of average daily market value
def calculate_net_flow_percent_adv(df, category):
    # Make an explicit copy of the DataFrame to avoid SettingWithCopyWarning
    df = df.copy()
    
    # Ensure necessary columns are present
    required_columns = {'PRC', 'SHROUT', 'date'}
    if not required_columns.issubset(df.columns):
        missing = required_columns - set(df.columns)
        raise ValueError(f"DataFrame is missing required columns: {missing}")
    
    # Calculate daily market value
    df.loc[:, 'market_value'] = df['PRC'] * df['SHROUT']  # Market Value = Price * Shares Outstanding
    
    # Calculate daily change in market value as a proxy for net flow
    df.loc[:, 'market_value_change'] = df['market_value'].diff().fillna(0)
    
    # Calculate average daily market value (ADM)
    adm = df['market_value'].mean()
    print(f"Average Daily Market Value for {category}: {adm}")  # Debugging step
    
    # If ADM is very small, set a minimum threshold to avoid huge percentages
    if adm < 1:
        adm = 1  # Adjusting to avoid division by extremely small ADM
    
    # Calculate daily net flow percentage relative to ADM
    df.loc[:, 'net_flow_percent'] = (df['market_value_change'] / adm) * 100
    
    # Add a category column
    df['Category'] = category
    
    # Group by date to avoid duplicate dates and sum daily net flow percentages
    df_grouped = df.groupby('date').agg({
        'net_flow_percent': 'sum'  # Sum net flow percentages for each date
    }).reset_index()
    
    # Add the category label back after grouping
    df_grouped['Category'] = category
    
    return df_grouped[['date', 'net_flow_percent', 'Category']]

# Filter data from January 2023 onward
bond_data = merged_data2[merged_data2['date'] >= '2023-01-01']

# Sample 204 equity funds for consistency with the number of corporate bonds
equity_sample_tickers = np.random.choice(CRSP_no_bonds['TICKER'].unique(), 204, replace=False)
equity_sample_data = CRSP_no_bonds[CRSP_no_bonds['TICKER'].isin(equity_sample_tickers)]
equity_data = equity_sample_data[equity_sample_data['date'] >= '2023-01-01']

# Calculate daily net flows as a percentage of average daily market value for each category
equity_net_flow_adv = calculate_net_flow_percent_adv(equity_data, 'Equity')
bond_net_flow_adv = calculate_net_flow_percent_adv(bond_data, 'Corporate Bond')

# Combine both into a single DataFrame for visualization
combined_data_adv = pd.concat([equity_net_flow_adv, bond_net_flow_adv])

# Ensure date is in datetime format
combined_data_adv['date'] = pd.to_datetime(combined_data_adv['date'], errors='coerce')
combined_data_adv = combined_data_adv.dropna(subset=['net_flow_percent', 'date'])  # Remove any NaN or infinite values

# Cap net_flow_percent values to make the chart readable
combined_data_adv['net_flow_percent'] = combined_data_adv['net_flow_percent'].clip(-100, 100)

# Test with only the first month of data for quick troubleshooting
test_data = combined_data_adv[(combined_data_adv['date'] >= '2023-02-16') & (combined_data_adv['date'] <= '2023-03-31')]

# Disable the max row limit in Altair for larger datasets
alt.data_transformers.disable_max_rows()

# Set a custom y-axis domain for a clearer view of daily changes
y_domain = [-100, 100]  # Adjust the domain to match the capped range

# Create the Altair chart with two lines, one for each category, showing daily net flow as a share of ADM
chart = alt.Chart(test_data).mark_line().encode(
    x=alt.X('date:T', title='Date', axis=alt.Axis(format='%b %d', tickMinStep=5)),
    y=alt.Y('net_flow_percent:Q', title='Daily Net Flow as % of Average Daily Market Value',
            scale=alt.Scale(domain=y_domain)),  # Set custom y-axis range
    color=alt.Color('Category:N', scale=alt.Scale(domain=['Equity', 'Corporate Bond'], range=['#2b6cb0', '#ff6961'])),
    tooltip=['date:T', 'net_flow_percent:Q', 'Category:N']
).properties(
    title='Daily Net Flow as Share of Average Daily Market Value (%)',
    width=600,
    height=400
)

# Display the chart
chart

# Optionally, save as HTML to view in a browser if still not rendering
#chart.save('net_flow_chart.html')

```

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.dates as mdates

# Load the data
data = pd.read_csv('quix1(Sheet1).csv')

# Remove rows with NA values
data = data.dropna()

# Convert the 'date ' column to datetime format (note the space)
data['date '] = pd.to_datetime(data['date '])

# Set the style
sns.set(style='whitegrid')

# Plot the data
plt.figure(figsize=(12, 6))

# Plot the Equity line in blue
sns.lineplot(x='date ', y='Equity', data=data, label='Equity', color='blue')

# Plot the Total Corporate Bond line in red
sns.lineplot(x='date ', y='Total Corporate Bond', data=data, label='Total Corporate Bond', color='red')

# Customize the date format on the x-axis
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format: Month Day

# Rotate date labels for better visibility
plt.xticks(rotation=45)

# Add a vertical line at March 13
plt.axvline(pd.to_datetime('2023-03-13'), color='gray', linestyle='--')

# Customize the plot
plt.title('Cumulative Net Flow as Share of Assets Under Management')
plt.xlabel('Date')
plt.ylabel('Percent')
plt.legend()
plt.grid(True)

# Set limits for scrolling (you can adjust these based on your data)
plt.xlim([data['date '].min(), data['date '].max()])  # Ensures the full date range is included

# Save the plot if needed
plt.savefig('your_plot.png')

# Show the plot
plt.show()
```

The fist graph show the Cumulative net Flows of both equity funds and corporate bonds. They do a comparison of the movement of both in order to see if it was the SVB failure that caused the movement or if there was another financial variables at play. They explain this by saying that when runs are experianced they are typically seen in corporate bonds and not in equity. As you can see both experience a negative movement. During this time their was a hike in interest rates which could also explain the downward trend in the bond market performance  







### For Chart 4
```{python}
# Skip the first 4 rows and set row 5 as the header
Long = pd.read_excel('Long.xlsx', skiprows=4)
```

```{python}
# Perform the merge to keep only matching tickers from the CSV file
# Assuming both datasets have the 'TICKER' column
merged_data3 = pd.merge(CRSP, Long[['TICKER']], on='TICKER', how='inner')

# Save the result to a new CSV file (optional)
merged_data3.to_csv('matched_CRSP_Long.csv', index=False)
```

```{python}
# Get the number of unique tickers in the TICKER column
num_unique_tickers = merged_data3['TICKER'].nunique()

# Print the result
print(f"Number of unique tickers: {num_unique_tickers}")
```

```{python}
# Skip the first 4 rows and set row 5 as the header
Short = pd.read_excel('Short.xlsx', skiprows=4)
```

```{python}
# Perform the merge to keep only matching tickers from the CSV file
# Assuming both datasets have the 'TICKER' column
merged_data4 = pd.merge(CRSP, Short[['TICKER']], on='TICKER', how='inner')

# Save the result to a new CSV file (optional)
merged_data4.to_csv('matched_CRSP_Short.csv', index=False)
```

```{python}
# Get the number of unique tickers in the TICKER column
num_unique_tickers = merged_data4['TICKER'].nunique()

# Print the result
print(f"Number of unique tickers: {num_unique_tickers}")
```

```{python}

# Function to calculate daily net flow as a percentage of average daily market value
def calculate_net_flow_percent_adv(df, category):
    # Make an explicit copy of the DataFrame to avoid SettingWithCopyWarning
    df = df.copy()
    
    # Ensure necessary columns are present
    required_columns = {'PRC', 'SHROUT', 'date'}
    if not required_columns.issubset(df.columns):
        missing = required_columns - set(df.columns)
        raise ValueError(f"DataFrame is missing required columns: {missing}")
    
    # Calculate daily market value
    df.loc[:, 'market_value'] = df['PRC'] * df['SHROUT']  # Market Value = Price * Shares Outstanding
    
    # Calculate daily change in market value as a proxy for net flow
    df.loc[:, 'market_value_change'] = df['market_value'].diff().fillna(0)
    
    # Calculate average daily market value (ADM)
    adm = df['market_value'].mean()
    print(f"Average Daily Market Value for {category}: {adm}")  # Debugging step
    
    # If ADM is very small, set a minimum threshold to avoid huge percentages
    if adm < 1:
        adm = 1  # Adjusting to avoid division by extremely small ADM
    
    # Calculate daily net flow percentage relative to ADM
    df.loc[:, 'net_flow_percent'] = (df['market_value_change'] / adm) * 100
    
    # Add a category column
    df['Category'] = category
    
    # Group by date to avoid duplicate dates and sum daily net flow percentages
    df_grouped = df.groupby('date').agg({
        'net_flow_percent': 'sum'  # Sum net flow percentages for each date
    }).reset_index()
    
    # Add the category label back after grouping
    df_grouped['Category'] = category
    
    return df_grouped[['date', 'net_flow_percent', 'Category']]

# Load and filter data for long and short bond categories
# Assuming `merged_data3` and `merged_data4` are your datasets for long and short term bonds respectively

# Filter data for the desired date range (February 16 to March 31)
start_date = '2023-02-16'
end_date = '2023-03-31'

# Calculate daily net flows as a percentage of average daily market value for each category
long_net_flow_adv = calculate_net_flow_percent_adv(merged_data3[(merged_data3['date'] >= start_date) & (merged_data3['date'] <= end_date)], 'Long')
short_net_flow_adv = calculate_net_flow_percent_adv(merged_data4[(merged_data4['date'] >= start_date) & (merged_data4['date'] <= end_date)], 'Short')

# Combine both into a single DataFrame for visualization
combined_data_adv = pd.concat([long_net_flow_adv, short_net_flow_adv])

# Ensure date is in datetime format
combined_data_adv['date'] = pd.to_datetime(combined_data_adv['date'], errors='coerce')
combined_data_adv = combined_data_adv.dropna(subset=['net_flow_percent', 'date'])  # Remove any NaN or infinite values

# Cap net_flow_percent values to make the chart readable
combined_data_adv['net_flow_percent'] = combined_data_adv['net_flow_percent'].clip(-100, 100)

# Disable the max row limit in Altair for larger datasets
alt.data_transformers.disable_max_rows()

# Set a custom y-axis domain for a clearer view of daily changes
y_domain = [-100, 100]  # Adjust the domain to match the capped range

# Create the Altair chart with two lines, one for each category, showing daily net flow as a share of ADM
chart = alt.Chart(combined_data_adv).mark_line().encode(
    x=alt.X('date:T', title='Date', axis=alt.Axis(format='%b %d', tickCount="day", tickMinStep=5)),  # 5-day interval
    y=alt.Y('net_flow_percent:Q', title='Daily Net Flow as % of Average Daily Market Value',
            scale=alt.Scale(domain=y_domain)),  # Set custom y-axis range
    color=alt.Color('Category:N', scale=alt.Scale(domain=['Long', 'Short'], range=['#2b6cb0', '#ff6961'])),
    tooltip=['date:T', 'net_flow_percent:Q', 'Category:N']
).properties(
    title='Daily Net Flow as Share of Average Daily Market Value (%)',
    width=600,
    height=400
)

# Display the chart
chart
```

The following graph is a graph of net flows for both short and long maturity bonds. Shorter maturity bonds are less affected by increase in interest rates than longer maturity bonds. By looking at the graph we see that we have higher outflows in the short maturity bond when compared to the longer maturity bonds. This can be to the expectation that interest rates will be lower in the future. 

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.dates as mdates

# Load the data
data = pd.read_csv('quick2(Sheet1).csv')

# Remove rows with NA values
data = data.dropna()

# Print the column names to confirm
print(data.columns)

# Strip any leading or trailing whitespace from column names
data.columns = data.columns.str.strip()

# Convert the 'date' column to datetime format
data['date'] = pd.to_datetime(data['date'])

# Set the style
sns.set(style='whitegrid')

# Plot the data
plt.figure(figsize=(12, 6))

# Plot the Long line in blue
sns.lineplot(x='date', y='Long', data=data, label='Long', color='blue')  # Note the space removed here

# Plot the Short line in red
sns.lineplot(x='date', y='Short', data=data, label='Short', color='red')

# Customize the date format on the x-axis
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))  # Format: Month Day

# Rotate date labels for better visibility
plt.xticks(rotation=45)

# Add a vertical line at March 13
plt.axvline(pd.to_datetime('2023-03-13'), color='gray', linestyle='--')

# Customize the plot
plt.title('Cumulative Net Flow as Share of Assets Under Management')
plt.xlabel('Date')
plt.ylabel('Percent')
plt.legend()
plt.grid(True)

# Set limits for scrolling
plt.xlim([data['date'].min(), data['date'].max()])  # Ensures the full date range is included

# Save the plot if needed
plt.savefig('your_plot.png')

# Show the plot
plt.show()

```